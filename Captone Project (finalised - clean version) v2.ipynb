{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d27046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ruamel-yaml\n",
    "!pip install nltk textstat\n",
    "!pip install PyPDF2\n",
    "!pip install pycryptodome\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install ipywidgets\n",
    "!pip install matplotlib\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2650e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the necessary library\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from PyPDF2 import PdfReader\n",
    "import textstat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "import string\n",
    "from tkinter import Tk, filedialog\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Import additional libraries for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8afbb",
   "metadata": {},
   "source": [
    "# Code to select file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe40e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [] \n",
    "\n",
    "def select_files(b):\n",
    "    global file_paths\n",
    "    root = Tk()\n",
    "    root.withdraw()  \n",
    "    file_paths = filedialog.askopenfilenames()  \n",
    "    print(file_paths) \n",
    "\n",
    "fileselect = widgets.Button(description=\"Select Files\")\n",
    "fileselect.on_click(select_files)\n",
    "\n",
    "display(fileselect)\n",
    "\n",
    "print(\"Please select the PDF files you wish to process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803bd9c",
   "metadata": {},
   "source": [
    "# Coding to check on optimum cluster number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF file given a path\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf = PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"  # Ensure we concatenate strings\n",
    "        return text\n",
    "\n",
    "# Function to calculate averages of the scores\n",
    "def calculate_averages(df):\n",
    "    return df.mean()\n",
    "\n",
    "# Function to find the optimal number of clusters using the Elbow Method and display WCSS table and plot\n",
    "def find_optimal_clusters(data, max_k=10):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    wcss = []\n",
    "    \n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(scaled_data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    # Display WCSS values in a table\n",
    "    wcss_df = pd.DataFrame({'Number of Clusters (k)': range(1, max_k + 1), 'WCSS': wcss})\n",
    "    display(wcss_df)\n",
    "    \n",
    "    # Plot the Elbow graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_k + 1), wcss, marker='o', linestyle='--')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "    plt.title('Elbow Method For Optimal Number of Clusters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Process the selected files\n",
    "def process_files(file_paths, n_clusters=3):\n",
    "    clear_output(wait=True)\n",
    "    results = []\n",
    "    for file_path in file_paths:\n",
    "        # Only process .pdf files\n",
    "        if file_path.endswith('.pdf'):\n",
    "            # Read the file content and extract text\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "\n",
    "            # Tokenize the text into sentences and words\n",
    "            sentences = sent_tokenize(text)\n",
    "            words = word_tokenize(text)\n",
    "\n",
    "            # Calculate readability scores\n",
    "            fk_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "            gunning_fog = textstat.gunning_fog(text)\n",
    "            flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "            smog_index = textstat.smog_index(text)\n",
    "            \n",
    "            # Append results for the current file\n",
    "            results.append({\n",
    "                'File': os.path.basename(file_path),\n",
    "                'Flesch-Kincaid Grade Level': fk_grade_level,\n",
    "                'Gunning Fog Index': gunning_fog,\n",
    "                'Flesch Reading Ease': flesch_reading_ease,\n",
    "                'SMOG Index': smog_index,\n",
    "            })\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate averages\n",
    "    averages = calculate_averages(df.iloc[:, 1:])\n",
    "    \n",
    "    # Perform clustering\n",
    "    features = df.iloc[:, 1:]  # Use all columns except the 'File' column for clustering\n",
    "    find_optimal_clusters(features, max_k=10)\n",
    "    \n",
    "    # Save the DataFrame with cluster labels\n",
    "    global clustered_data\n",
    "    clustered_data = df.copy()\n",
    "\n",
    "# Add a button to trigger file processing after selection\n",
    "process_button = widgets.Button(description=\"Process Files\")\n",
    "process_button.on_click(lambda b: process_files(file_paths))\n",
    "display(process_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def59fa",
   "metadata": {},
   "source": [
    " # Coding to apply Readability Formula and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f584d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF file given a path\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf = PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"  # Ensure we concatenate strings\n",
    "        return text\n",
    "\n",
    "# Function to calculate averages of the scores\n",
    "def calculate_averages(df):\n",
    "    return df.mean()\n",
    "\n",
    "\n",
    "# Process the selected files\n",
    "def process_files(file_paths, n_clusters=3):\n",
    "    clear_output(wait=True)\n",
    "    results = []\n",
    "    for file_path in file_paths:\n",
    "        # Only process .pdf files\n",
    "        if file_path.endswith('.pdf'):\n",
    "            # Read the file content and extract text\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "\n",
    "            # Tokenize the text into sentences and words\n",
    "            sentences = sent_tokenize(text)\n",
    "            words = word_tokenize(text)\n",
    "\n",
    "            # Calculate readability scores\n",
    "            fk_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "            gunning_fog = textstat.gunning_fog(text)\n",
    "            flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "            smog_index = textstat.smog_index(text)\n",
    "\n",
    "\n",
    "            # Perform sentiment analysis\n",
    "            blob = TextBlob(text)\n",
    "            polarity, subjectivity = blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "            \n",
    "            # Append results for the current file\n",
    "            results.append({\n",
    "                'File': os.path.basename(file_path),\n",
    "                'Flesch-Kincaid Grade Level': fk_grade_level,\n",
    "                'Gunning Fog Index': gunning_fog,\n",
    "                'Flesch Reading Ease': flesch_reading_ease,\n",
    "                'SMOG Index': smog_index,\n",
    "            })\n",
    "    \n",
    "    # Convert the results to a DataFrame and display it\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate averages\n",
    "    averages = calculate_averages(df.iloc[:, 1:])\n",
    "    \n",
    "    # Perform clustering\n",
    "    features = df.iloc[:, 1:-2]  # Exclude ASL and ASW from clustering features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    df['Cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Set pandas options to display all rows\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    # Display the DataFrame\n",
    "    display(df)\n",
    "    \n",
    "    # Save the DataFrame with cluster labels\n",
    "    global clustered_data\n",
    "    clustered_data = df.copy()\n",
    "\n",
    "    # Display averages\n",
    "    print(\"Average Scores:\")\n",
    "    display(averages)\n",
    "    \n",
    "    \n",
    "# Add a button to trigger file processing after selection\n",
    "process_button = widgets.Button(description=\"Process Files\")\n",
    "process_button.on_click(lambda b: process_files(file_paths))\n",
    "display(process_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642390b5",
   "metadata": {},
   "source": [
    " # Coding for statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab760e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display statistical summary\n",
    "def display_statistical_summary(df):\n",
    "    summary = df.describe().transpose()\n",
    "    display(summary)\n",
    "\n",
    "# Function to plot histograms for readability scores with mean labels\n",
    "def plot_histograms(df):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Exclude 'File', 'Cluster', and 'Dale-Chall Score' columns\n",
    "    metrics_to_plot = [\n",
    "        'Flesch-Kincaid Grade Level',\n",
    "        'Gunning Fog Index',\n",
    "        'Flesch Reading Ease',\n",
    "        'SMOG Index'\n",
    "    ]\n",
    "    \n",
    "    for idx, col in enumerate(metrics_to_plot):\n",
    "        sns.histplot(df[col], ax=axes[idx], kde=True)\n",
    "        mean_val = df[col].mean()\n",
    "        axes[idx].axvline(mean_val, color='r', linestyle='--')\n",
    "        axes[idx].text(mean_val, axes[idx].get_ylim()[1] * 0.9, f'Mean: {mean_val:.2f}', color='r')\n",
    "        axes[idx].set_title(f'Histogram of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (make sure df is loaded)\n",
    "# Assume df is the DataFrame created from the process_files function\n",
    "\n",
    "# Load your DataFrame (clustered_data) and use it for analysis\n",
    "def load_and_analyze():\n",
    "    global clustered_data  # Assuming clustered_data is the DataFrame from the previous part\n",
    "    df = clustered_data.copy()  # Make sure this DataFrame is already created\n",
    "\n",
    "    # Display statistical summary\n",
    "    display_statistical_summary(df.iloc[:, 1:])\n",
    "    \n",
    "    # Plot histograms\n",
    "    plot_histograms(df)\n",
    "\n",
    "# Button to trigger analysis\n",
    "analyze_button = widgets.Button(description=\"Analyze Data\")\n",
    "analyze_button.on_click(lambda b: load_and_analyze())\n",
    "display(analyze_button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25032d5b",
   "metadata": {},
   "source": [
    " # Coding to check number of documents in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4da90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and display the number of documents in each cluster\n",
    "def calculate_and_display_cluster_counts():\n",
    "    if 'clustered_data' in globals():\n",
    "        cluster_counts = clustered_data['Cluster'].value_counts().sort_index()\n",
    "        print(\"Number of documents in each cluster:\")\n",
    "        print(cluster_counts)\n",
    "    else:\n",
    "        print(\"Data not found. Please run the data processing cell first.\")\n",
    "\n",
    "# Add a button to trigger cluster count calculation\n",
    "cluster_count_button = widgets.Button(description=\"Calculate Cluster Counts\")\n",
    "cluster_count_button.on_click(lambda b: calculate_and_display_cluster_counts())\n",
    "display(cluster_count_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab1b3f",
   "metadata": {},
   "source": [
    " # Coding to visualise Box-Plot by each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ca5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this cell is run after the data processing cell\n",
    "def generate_boxplots(output_dir='output_plots'):\n",
    "    if 'clustered_data' in globals():\n",
    "        scores = ['Flesch-Kincaid Grade Level', 'Gunning Fog Index', 'Flesch Reading Ease', 'SMOG Index']\n",
    "        for score in scores:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            ax = sns.boxplot(x='Cluster', y=score, data=clustered_data)\n",
    "            plt.title(f'Boxplot of {score} by Cluster')\n",
    "\n",
    "            # Calculate means and add them as text annotations\n",
    "            means = clustered_data.groupby('Cluster')[score].mean()\n",
    "            for index, mean in enumerate(means):\n",
    "                ax.text(index, mean, f'{mean:.2f}', color='black', ha=\"center\")\n",
    "\n",
    "            # Check if output directory exists, if not, create it\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            # Save the figure before showing it\n",
    "            plt.savefig(f\"{output_dir}/{score.replace(' ', '_')}_Boxplot.png\")\n",
    "\n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Data not found. Please run the data processing cell first.\")\n",
    "\n",
    "# Call the function to generate and save boxplots\n",
    "generate_boxplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef901841",
   "metadata": {},
   "source": [
    " # Coding for summary statistics by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers(data):\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "\n",
    "# Function to generate and display enhanced summary statistics for each cluster\n",
    "def generate_and_display_summary_statistics():\n",
    "    if 'clustered_data' in globals():\n",
    "        summary_statistics = clustered_data.groupby('Cluster').agg({\n",
    "            'Flesch-Kincaid Grade Level': ['mean', 'median', 'std', 'min', 'max', detect_outliers],\n",
    "            'Gunning Fog Index': ['mean', 'median', 'std', 'min', 'max', detect_outliers],\n",
    "            'Flesch Reading Ease': ['mean', 'median', 'std', 'min', 'max', detect_outliers],\n",
    "            'SMOG Index': ['mean', 'median', 'std', 'min', 'max', detect_outliers]\n",
    "        }).reset_index()\n",
    "\n",
    "        # Flatten the MultiIndex columns\n",
    "        summary_statistics.columns = [' '.join(col).strip() for col in summary_statistics.columns.values]\n",
    "        \n",
    "        # Rename the outliers column for clarity\n",
    "        summary_statistics = summary_statistics.rename(columns={\n",
    "            'Flesch-Kincaid Grade Level detect_outliers': 'Flesch-Kincaid Grade Level Outliers',\n",
    "            'Gunning Fog Index detect_outliers': 'Gunning Fog Index Outliers',\n",
    "            'Flesch Reading Ease detect_outliers': 'Flesch Reading Ease Outliers',\n",
    "            'SMOG Index detect_outliers': 'SMOG Index Outliers'\n",
    "        })\n",
    "\n",
    "        # Display the summary statistics\n",
    "        display(summary_statistics)\n",
    "    else:\n",
    "        print(\"Data not found. Please run the data processing cell first.\")\n",
    "\n",
    "# Call the function to generate and display summary statistics\n",
    "generate_and_display_summary_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2091759",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deba4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4850500",
   "metadata": {},
   "source": [
    " # Coding to check optimum number for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9db899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Range of n values to test\n",
    "n_neighbors = range(1, 10)\n",
    "\n",
    "# List to store cross-validation scores\n",
    "cv_scores = []\n",
    "\n",
    "for k in n_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# Find the best k\n",
    "best_k = n_neighbors[cv_scores.index(max(cv_scores))]\n",
    "print(f\"Best number of neighbors: {best_k}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))  # Adjust figure size for better visibility\n",
    "plt.plot(n_neighbors, cv_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.title('KNN Varying Number of Neighbors')\n",
    "plt.annotate(f'Best k = {best_k}', \n",
    "             xy=(best_k, max(cv_scores)), \n",
    "             xytext=(best_k + 0.3, max(cv_scores) - 0.03),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->', lw=1.5),\n",
    "             fontsize=10, ha='center')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "plot_path = 'C:/Users/User/OneDrive/Documents/Along/Master/STQD6889 Capstone Project/Part 2/knn_varying_neighbors.png'\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "# Display the path where the plot is saved\n",
    "plot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584597ec",
   "metadata": {},
   "source": [
    " # Coding for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the target and feature columns\n",
    "target = 'Cluster'\n",
    "features = clustered_data.drop(columns=[target, 'File'])  # Exclude 'File' and 'Cluster' columns\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, clustered_data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Train a K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Train an XGBoost Classifier\n",
    "xgboost = xgb.XGBClassifier(random_state=42)\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the Decision Tree Classifier\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "tree_accuracy = accuracy_score(y_test, y_pred_tree)\n",
    "tree_report = classification_report(y_test, y_pred_tree)\n",
    "\n",
    "# Predict and evaluate the Random Forest Classifier\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "forest_accuracy = accuracy_score(y_test, y_pred_forest)\n",
    "forest_report = classification_report(y_test, y_pred_forest)\n",
    "\n",
    "# Predict and evaluate the K-Nearest Neighbors Classifier\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "knn_report = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "# Predict and evaluate the XGBoost Classifier\n",
    "y_pred_xgb = xgboost.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_report = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Decision Tree Classifier Accuracy:\", tree_accuracy)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(tree_report)\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Accuracy:\", forest_accuracy)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(forest_report)\n",
    "\n",
    "print(\"K-Nearest Neighbors Classifier Accuracy:\", knn_accuracy)\n",
    "print(\"K-Nearest Neighbors Classification Report:\")\n",
    "print(knn_report)\n",
    "\n",
    "print(\"\\nXGBoost Classifier Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(xgb_report)\n",
    "\n",
    "\n",
    "#paper support kenapa kita pilih random forest (bagging), decision tree, KNN - traditional method\n",
    "#try XGBoost (boosting), stack generalisation machine learning (combination) - ensemble method\n",
    "#paper support accuracy, precision, recall, f1-score\n",
    "#cari features importance among dalam score/characteristic, random forest & decision tree almost the same. tapi KNN lain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29014ba5",
   "metadata": {},
   "source": [
    " # Coding to check on feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "rf_importances = random_forest.feature_importances_\n",
    "rf_importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': rf_importances})\n",
    "rf_importance_df = rf_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"Random Forest Feature Importances:\")\n",
    "print(rf_importance_df)\n",
    "\n",
    "# Decision Tree Feature Importance\n",
    "dt_importances = decision_tree.feature_importances_\n",
    "dt_importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': dt_importances})\n",
    "dt_importance_df = dt_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nDecision Tree Feature Importances:\")\n",
    "print(dt_importance_df)\n",
    "\n",
    "# K-Nearest Neighbors Feature Importance using Permutation Importance\n",
    "knn_importance_result = permutation_importance(knn, X_test, y_test, n_repeats=10, random_state=42)\n",
    "knn_importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': knn_importance_result.importances_mean})\n",
    "knn_importance_df = knn_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nK-Nearest Neighbors Feature Importances:\")\n",
    "print(knn_importance_df)\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "xgb_importances = xgboost.feature_importances_\n",
    "xgb_importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': xgb_importances})\n",
    "xgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(xgb_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57e1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
